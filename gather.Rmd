---
title: "gather"
author: "Arushi Saxena"
date: "4/03/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#install.packages("rvest")
#install.packages("tidytext")
#install_github("lchiffon/wordcloud2")
#install.packages("wordcloud2")
#install.packages("Rcpp")
#install.packages("rjson")
#install.packages("twitteR")
library(Rcpp)
library(dplyr)
library(ggplot2)
library(reprex)
library(tidyverse)
library(readxl)
library(janitor)
library(gt)
library(rvest)
library(stringr)
library(shiny)
library(rvest)
library(lubridate)
library(hms)
require(devtools)
library(tidytext)
library(rtweet)
library(rjson)
library(jsonlite)
library(twitteR)
```

```{r save this file locally, echo=FALSE, include=FALSE}
a <- read.csv(file="https://raw.githubusercontent.com/fivethirtyeight/russian-troll-tweets/master/IRAhandle_tweets_1.csv") 
```
```{r}
saveRDS(a, file ="RussianTweets.RDS")
RussianTweets <- readRDS("RussianTweets.RDS")

RussianTweets
getwd()

filelist <- list.files(path="C:/Users/arush/Downloads/russian-troll-tweets-master/russian-troll-tweets-master", pattern="*.csv")

for(i in 1:5) {
file<-read_csv(paste0("C:/Users/arush/Downloads/russian-troll-tweets-master/russian-troll-tweets-master/", filelist[i]))
if(i==1){
  TweetFrame<-file
  next
}
TweetFrame<-bind_rows(TweetFrame, file)
 
 }

```

```{r}

# Split the publish date timestap into data and time for ease of use

TweetFrame1 <- TweetFrame %>%
  mutate(date = sapply(strsplit(as.character(TweetFrame$publish_date), " "), "[", 1)) %>%
  mutate(time = sapply(strsplit(as.character(TweetFrame$publish_date), " "), "[", 2))

# Make a new df with English only content from the year 2016. Use 'language' and 'publish_date'

cleaned <- TweetFrame1 %>%
  select(language, date, content) %>%
  filter(language == "English") %>%
  filter(stringr::str_detect(date, '2016'))

# Making the wordcloud utilizing this link https://www.r-bloggers.com/awesome-twitter-word-clouds-in-r/

# Unnest the words for cloud - code via Tidy Text

trolltweets <- cleaned %>% 
  unnest_tokens(word, content)

# Remove stop words - aka typically very common words such as "the", "of" etc

data(stop_words)
trolltweets <- trolltweets %>%
  anti_join(stop_words)

# Remove other nonsense words

trolltweets2 <- trolltweets %>%
  filter(!word %in% c('t.co', 'https'))

# Do a word count for the word cloud

trolltweets3 <- trolltweets2 %>%
  count(word, sort = TRUE)

trolltweets3
  
# This generates the Word Cloud using WordCloud2 library
wordcloud2(trolltweets3, size=0.7)

```

# Scraping Twitter Data directly
```{r}
## search for 1000 tweets using the #socialdistancing hashtag https://rtweet.info/

rt <- search_tweets(
  "#socialdistancing", n = 15000, include_rts = FALSE
)

rt

rt2 <- stream_tweets("")
```

```{r downloading json zip file}
# https://www.stat.berkeley.edu/~paciorek/computingTips/Reading_gzipped_bzipped_zip.html


gunzip("data.json.gz", remove=FALSE)
us2 <- jsonlite::fromJSON("data.json")
us2
```

```{r Twitter authentication set up}
consumer_key <- '5SP9pW4kckAMgSz1FD8htZssO'
consumer_secret <- 'TJZkjzhc53piCXreADGSEDkz9tYACsWNExu7kXiGZxGCLaUFHd'
access_token <-'790715299-jSaOQZohj6OvFVoXGeXMg61i1lAqAzLkOq9SL1Gi'
access_secret <-'awcZ4arCLIg56RJ8PrU87CzbKZKF8as836MzWuJuVR9UX'

setup_twitter_oauth(consumer_key, consumer_secret, access_token, access_secret)
2
```


