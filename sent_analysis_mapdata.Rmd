---
title: "sentiment_analysis_mapdata"
author: "Arushi Saxena"
date: "4/15/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(twitteR)
library(rtweet)
library(SnowballC)
library(tm)
library(syuzhet)
library(maps)
library(ggthemes)
library(revgeo)
library(mgsub)
library(lubridate)
```


```{r Tweet Sentiment Analysis}
#Text contains a lot of URLs, hashtags and other twitter handles. We will remove all these using the gsub function.

#Convert original RDS into data for sentiment analysis
senti <- tweets_0412_v2
head(senti)

head(senti$text)
senti <- gsub("http.*","",senti$text)
senti <- gsub("https.*","",senti)
senti <- gsub("#.*","",senti)
senti <- gsub("@.*","",senti)

#Getting sentiment defining words
word <- as.vector(senti)
emotion <- get_nrc_sentiment(word)
emotion2 <- cbind(senti, emotion) 

head(emotion2)

#Getting most positive sentiment
sent.value <- get_sentiment(word)
most.positive <- word[sent.value == max(sent.value)]
most.positive

#Getting most negative sentiment
most.negative <- word[sent.value <= min(sent.value)] 
most.negative 

#Make a tibble of scores by Twitter row ID
senti
sent.value

sentiment_scores <- tibble(manual_id = rep(1:18000),sent.value) 

sentiment_scores

#Add Twitter Row IDs to original df
twitter_sentiment_table <- tweets_0412_v2 %>%
  mutate(manual_id = rep(1:18000)) %>%
  select(manual_id,text,country,geo_coords,coords_coords,bbox_coords,location)

twitter_sentiment_table

#Join sentiment score to the Twitter Sentiment Table on manual_id

twitter_sentiment_table2 <- left_join(twitter_sentiment_table, sentiment_scores, by = "manual_id")

head(twitter_sentiment_table2)

#Make a US heatmap of the sentiment scores
senti_locations <- lat_lng(twitter_sentiment_table2)

## plot state boundaries
par(mar = c(0, 0, 0, 0))
map <- maps::map("state", lwd = 1) %>%
with(senti_locations, points(lng, lat, pch = 20, cex = 5, col = rgb(0, .3, .7, .75)))

# create basemap of the globe (Source: https://www.earthdatascience.org/courses/earth-analytics/get-data-using-apis/map-tweet-locations-over-time-r/)

world_basemap <- ggplot() +
  borders("world", colour = "gray85", fill = "gray80") + theme_map()

#plot data onto world map
world_basemap +
  geom_point(data = senti_locations, aes(x = lng, y = lat, fill = sent.value),
             , alpha = .5) +
  scale_size_continuous(range = c(1, 8),
                        breaks = c(250, 500, 750, 1000)) 


# This assumes that people are tweeting from where they live, but might not always be the case
  
us_senti_locations <- senti_locations %>%
  filter(lat != "" & lng !="") %>%
  filter(country == "United States")

us_senti_locations

#Prepping the US social distancing data for the heat map
us2 %>%
  distinct(state,county,.keep_all = TRUE)

#Only look at social distancing data after 3/11 (COVID-19 declared as pandemic) and average the values accross counties and categories to get an average state level view.

# Add Washington DC data manually from another source because it was missing in this web scraping

socialdistance_march <- us2 %>%
  mutate(date_formatted = ymd(date)) %>%
  filter(date_formatted >= "2020-03-11") %>%
  group_by(state) %>%
  summarize(average = mean(value)) %>%
  arrange(average) %>%
  add_row(state = "District of Columbia", average = "-17.259")
```

