---
title: "covid_datacollection"
author: "Arushi Saxena"
date: "4/10/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#install.packages("rvest")
#install.packages("tidytext")
#install_github("lchiffon/wordcloud2")
#install.packages("wordcloud2")
#install.packages("Rcpp")
#install.packages("rjson")
#install.packages("httpuv")
#install.packages("revgeo") #for reverse geocoding tweet location
#install.packages("mgsub") #to clean location data and replace strings
#install.packages("AlphaPart") #to turn tweets df into excel file
#installed.packages("SnowballC") #textmining
#installed.packages("tm")  #textmining
#installed.packages("syuzhet") #sentiment analysis

library(Rcpp)
library(dplyr)
library(ggplot2)
library(reprex)
library(tidyverse)
library(readxl)
library(janitor)
library(gt)
library(rvest)
library(stringr)
library(shiny)
library(rvest)
library(lubridate)
library(hms)
require(devtools)
library(tidytext)
library(rtweet)
library(rjson)
library(jsonlite)
library(httpuv)
library(maps)
library(ggthemes)
library(revgeo)
library(mgsub)
library(devtools)
library(twitteR)
library(SnowballC)
library(tm)
library(syuzhet)

```

# Scraping Twitter Data directly
```{r Twitter random scrape using Rtweet}
## search for 15000 tweets using the #socialdistancing hashtag https://rtweet.info/

## store api keys
api_key <- "5SP9pW4kckAMgSz1FD8htZssO"
api_secret_key <-"TJZkjzhc53piCXreADGSEDkz9tYACsWNExu7kXiGZxGCLaUFHd"

## authenticate via web browser for rtweet
token <- create_token(
  app = "GOV1005 - Twitter & COVID-19",
  consumer_key = api_key,
  consumer_secret = api_secret_key)

get_token()

rt_041220 <- search_tweets(
  "social distancing", n = 18000, include_rts = FALSE, geocode = lookup_coords("usa"), retryonratelimit = TRUE
)

#Turn tweets into a dataframe
tweets_0412 <-as.data.frame(rt_041220)
tweets_0412

#Write dataframe into an RDS for later analysis
saveRDS(tweets_0412, file = "tweets_0412.rds")
tweets_0412_v2 <- readRDS(file = "tweets_0412.rds")

#Write dataframe into a CSV for analysis
write_as_csv(tweets_0412, "tweets_0412_v2.csv", prepend_ids = TRUE, na = "", fileEncoding = "UTF-8")

#Read in the Twitter CSV to analyze
tweets <- read_csv("raw-data/tweets_0412_v2.csv")

#Other query - DID NOT USE
rt <- search_tweets(
  "#socialdistancing", geocode = lookup_coords("usa"), n = 10000)
```

```{r setup using TwitteR}
api_key <- "5SP9pW4kckAMgSz1FD8htZssO"
 
api_secret <- "TJZkjzhc53piCXreADGSEDkz9tYACsWNExu7kXiGZxGCLaUFHd"
 
access_token <-"790715299-NrNEPZWNIazaLJ4ZkZOJqfNARzmLPMmetV46DLoe"
 
access_token_secret <-"lcxRlQAjKl4XUzhnWfukzGD5MPnotKbM6V3jkbQBCD4Jh"
 
setup_twitter_oauth(api_key,api_secret, access_token,access_token_secret)

searchTwitter('social distancing', since='2020-03-25', until='2020-03-28', n=25000)
```


```{r Tweet gelocation mapping attempt 1 - Rtweet}
## create lat/lng variables using all available tweet and profile geo-location data. Cannot use excel file for this must use twitter dataframe or RDS due because they retain character types

rt2 <- lat_lng(rt)
test3 <- lat_lng(tweets_0412_v2)


glimpse(test3)

## plot state boundaries
par(mar = c(0, 0, 0, 0))
map <- maps::map("state", lwd = 1) %>%
with(rt2, points(lng, lat, pch = 20, cex = 5, col = rgb(0, .3, .7, .75)))

# create basemap of the globe (Source: https://www.earthdatascience.org/courses/earth-analytics/get-data-using-apis/map-tweet-locations-over-time-r/)

world_basemap <- ggplot() +
  borders("world", colour = "gray85", fill = "gray80") + theme_map()

world_basemap

tweet_locations <- rt2 %>%
  select(lat,lng) %>%
    na.omit()

tweet_locations3 <- test3 %>%
  select(lat,lng) %>%
    na.omit()


world_basemap +
  geom_point(data = tweet_locations3, aes(x = lng, y = lat),
             colour = 'purple', alpha = .5) +
  scale_size_continuous(range = c(1, 8),
                        breaks = c(250, 500, 750, 1000)) 

```

```{r Tweet gelocation mapping attempt 2}

#Clean Location Data: remove  blank locations and replace all % with blank so that geocode() will work https://www.r-bloggers.com/mapping-twitter-followers-in-r/

head(rt2$location, 10)

rt3 <- rt2 %>%
  filter(location !="") %>%
  mutate(location2 = str_replace_all(location,"%"," "))

tweets_0412_v2 <- tweets_0412 %>%
  filter(location !="") %>%
  mutate(location2 = str_replace_all(location,"%"," "))


#Install key package helpers:
source("https://raw.githubusercontent.com/LucasPuente/geocoding/master/geocode_helpers.R")
#Install modified version of the geocode function
#(that now includes the api_key parameter):
source("https://raw.githubusercontent.com/LucasPuente/geocoding/master/modified_geocode.R")

geocode_apply<-function(x){
  geocode(x, source = "google", output = "all", api_key="AIzaSyCikMlMJLBSRbEr2-zH1lRkyqmrNJ_FjuE")
}


geocode_results_0412<-sapply(tweets_0412_v2$location2, geocode_apply, simplify = F)

geocode_results_0412

condition_a <- sapply(geocode_results, function(x) x["status"]=="OK")
geocode_results<-geocode_results[condition_a]

length(geocode_results)
```

```{r Tweet Sentiment Analysis}
#Text contains a lot of URLs, hashtags and other twitter handles. We will remove all these using the gsub function.

#Convert original RDS into data for sentiment analysis
senti <- tweets_0412_v2

head(senti$text)
senti <- gsub("http.*","",senti$text)
senti <- gsub("https.*","",senti)
senti <- gsub("#.*","",senti)
senti <- gsub("@.*","",senti)

#Getting sentiment defining words
word <- as.vector(senti)
emotion <- get_nrc_sentiment(word)
emotion2 <- cbind(senti, emotion) 

head(emotion2)
```

